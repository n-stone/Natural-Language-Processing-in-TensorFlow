{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOINjKPrinHf8lKFBrDocqw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"NMM0fVUBcUTq","executionInfo":{"status":"ok","timestamp":1673836776540,"user_tz":-540,"elapsed":3211,"user":{"displayName":"정동명","userId":"08707986978784675546"}}},"outputs":[],"source":["import csv\n","import random\n","import pickle\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import matplotlib.pyplot as plt\n","from scipy.stats import linregress"]},{"cell_type":"code","source":["EMBEDDING_DIM = 100\n","MAXLEN = 16\n","TRUNCATING = 'post'\n","PADDING = 'post'\n","OOV_TOKEN = \"<OOV>\"\n","MAX_EXAMPLES = 160000\n","TRAINING_SPLIT = 0.9"],"metadata":{"id":"1bWxglercZFA","executionInfo":{"status":"ok","timestamp":1673836776541,"user_tz":-540,"elapsed":6,"user":{"displayName":"정동명","userId":"08707986978784675546"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["SENTIMENT_CSV = \"./data/training_cleaned.csv\"\n","\n","with open(SENTIMENT_CSV, 'r') as csvfile:\n","    print(f\"First data point looks like this:\\n\\n{csvfile.readline()}\")\n","    print(f\"Second data point looks like this:\\n\\n{csvfile.readline()}\")"],"metadata":{"id":"r-MinO81caYH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_data_from_file(filename):\n","    sentences = []\n","    labels = []\n","    \n","    with open(filename, 'r') as csvfile:\n","        reader = csv.reader(csvfile, delimiter=\",\")\n","        item_list = list(reader)\n","\n","        sentences = [ item[5] for item in item_list ]\n","        labels = [ 0 if item[0] is '0' else 1 for item in item_list ]\n","\n","    return sentences, labels"],"metadata":{"id":"cttvnbuHcqEy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences, labels = parse_data_from_file(SENTIMENT_CSV)\n","\n","print(f\"dataset contains {len(sentences)} examples\\n\")\n","\n","print(f\"Text of second example should look like this:\\n{sentences[1]}\\n\")\n","print(f\"Text of fourth example should look like this:\\n{sentences[3]}\")\n","\n","print(f\"\\nLabels of last 5 examples should look like this:\\n{labels[-5:]}\")"],"metadata":{"id":"6HmYnXmccvFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences_and_labels = list(zip(sentences, labels))\n","\n","random.seed(42)\n","sentences_and_labels = random.sample(sentences_and_labels, MAX_EXAMPLES)\n","\n","sentences, labels = zip(*sentences_and_labels)\n","\n","print(f\"There are {len(sentences)} sentences and {len(labels)} labels after random sampling\\n\")"],"metadata":{"id":"Aw8z-Z7acwTI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_val_split(sentences, labels, training_split):\n","    train_size = int(len(sentences) * training_split)\n","    train_sentences = sentences[:train_size]\n","    train_labels = labels[:train_size]\n","\n","    validation_sentences = sentences[train_size:]\n","    validation_labels = labels[train_size:]\n","    \n","    return train_sentences, validation_sentences, train_labels, validation_labels"],"metadata":{"id":"MXoc8LkzcywP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)\n","\n","print(f\"There are {len(train_sentences)} sentences for training.\\n\")\n","print(f\"There are {len(train_labels)} labels for training.\\n\")\n","print(f\"There are {len(val_sentences)} sentences for validation.\\n\")\n","print(f\"There are {len(val_labels)} labels for validation.\")"],"metadata":{"id":"Nx9KLNLfc2Eg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fit_tokenizer(train_sentences, oov_token):\n","    tokenizer = Tokenizer(oov_token = OOV_TOKEN)\n","    tokenizer.fit_on_texts(train_sentences)\n","    \n","    return tokenizer"],"metadata":{"id":"2xdsmXwyc3RR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = fit_tokenizer(train_sentences, OOV_TOKEN)\n","\n","word_index = tokenizer.word_index\n","VOCAB_SIZE = len(word_index)\n","\n","print(f\"Vocabulary contains {VOCAB_SIZE} words\\n\")\n","print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")\n","print(f\"\\nindex of word 'i' should be {word_index['i']}\")"],"metadata":{"id":"TtCI0406c7J4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seq_pad_and_trunc(sentences, tokenizer, padding, truncating, maxlen):\n","    sequences = tokenizer.texts_to_sequences(sentences)\n","    \n","    pad_trunc_sequences = pad_sequences(sequences,maxlen=MAXLEN, truncating=TRUNCATING)\n","    \n","    return pad_trunc_sequences"],"metadata":{"id":"5mm37OoJc9x4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_pad_trunc_seq = seq_pad_and_trunc(train_sentences, tokenizer, PADDING, TRUNCATING, MAXLEN)\n","val_pad_trunc_seq = seq_pad_and_trunc(val_sentences, tokenizer, PADDING, TRUNCATING, MAXLEN)\n","\n","print(f\"Padded and truncated training sequences have shape: {train_pad_trunc_seq.shape}\\n\")\n","print(f\"Padded and truncated validation sequences have shape: {val_pad_trunc_seq.shape}\")"],"metadata":{"id":"vr3lnaQ9c92Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_labels = np.array(train_labels)\n","val_labels = np.array(val_labels)"],"metadata":{"id":"L9BALNpddDAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["GLOVE_FILE = './data/glove.6B.100d.txt'\n","\n","GLOVE_EMBEDDINGS = {}\n","\n","with open(GLOVE_FILE) as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        GLOVE_EMBEDDINGS[word] = coefs"],"metadata":{"id":"zyLfIigLdDPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_word = 'dog'\n","test_vector = GLOVE_EMBEDDINGS[test_word]\n","\n","print(f\"Vector representation of word {test_word} looks like this:\\n\\n{test_vector}\")\n","print(f\"Each word vector has shape: {test_vector.shape}\")"],"metadata":{"id":"CV5UyJKVdFRx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EMBEDDINGS_MATRIX = np.zeros((VOCAB_SIZE+1, EMBEDDING_DIM))\n","\n","for word, i in word_index.items():\n","    embedding_vector = GLOVE_EMBEDDINGS.get(word)\n","    if embedding_vector is not None:\n","        EMBEDDINGS_MATRIX[i] = embedding_vector"],"metadata":{"id":"Myu6zC3mdJNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n","    model = tf.keras.Sequential([ \n","        tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=maxlen, weights=[embeddings_matrix], trainable=False),\n","        tf.keras.layers.Dropout(0.2),\n","        tf.keras.layers.Conv1D(64, 5, activation='relu'),\n","        tf.keras.layers.MaxPooling1D(pool_size=4),\n","        tf.keras.layers.LSTM(64),\n","        tf.keras.layers.Dense(1, activation='sigmoid')\n","    ])\n","    \n","    model.compile(loss='binary_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy']) \n","\n","    return model"],"metadata":{"id":"NFemhVkidG5B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = create_model(VOCAB_SIZE, EMBEDDING_DIM, MAXLEN, EMBEDDINGS_MATRIX)\n","\n","history = model.fit(train_pad_trunc_seq, train_labels, epochs=20, validation_data=(val_pad_trunc_seq, val_labels))"],"metadata":{"id":"EbNiSMqodRGB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = [*range(20)]\n","\n","plt.plot(epochs, loss, 'r')\n","plt.plot(epochs, val_loss, 'b')\n","plt.title('Training and validation loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend([\"Loss\", \"Validation Loss\"])\n","plt.show()"],"metadata":{"id":"BetPAxNJdR4J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","plt.plot(epochs, acc, 'r')\n","plt.plot(epochs, val_acc, 'b')\n","plt.title('Training and validation accuracy')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n","plt.show()"],"metadata":{"id":"xLaS9Pz6dUeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["slope, *_ = linregress(epochs, val_loss)\n","print(f\"The slope of your validation loss curve is {slope:.5f}\")"],"metadata":{"id":"-BWf4-7vdWA5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('history.pkl', 'wb') as f:\n","    pickle.dump(history.history, f)"],"metadata":{"id":"DaO_N6o6dXUo"},"execution_count":null,"outputs":[]}]}